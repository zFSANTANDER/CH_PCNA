{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"Imagenes/Logo Arauco 1.png\" style=\"width: 100px\">\n",
    "<img align=\"left\" src=\"Imagenes/Logo Arauco 2.png\" style=\"width: 100px\">\n",
    "\n",
    "   # <center>     Notebook Detección Corte de hoja </center>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook es creado para el entrenamiento de red neuronal para detección de corte de hoja de maquina L2.\n",
    "Es importante que cada vez que se realicen cambios al codigo queden documentados indicando:\n",
    "- Razon de la modificación\n",
    "- Fecha de modificación\n",
    "- Quien ejecuto la modificación\n",
    "\n",
    "Lo que buscamos con utilizar este notebook es que todos podamos documentar y aportar en la creación del modelo.\n",
    "Los integrantes del equipo de trabajo son:\n",
    "- Leonardo Bravo\n",
    "- Victor Encina\n",
    "- Yamil Avello\n",
    "- Guillermo Viñas\n",
    "- Felipe Santander\n",
    "\n",
    "El Notebook fue creado el 10.09.2019.\n",
    "\n",
    "1-. Instalado modulo de TensorFlow (10.09.2019)\n",
    "\n",
    "<rigth> **Felipe Santander** </right>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación de SW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANTE: Instalar tensorflow y pandas al correr por primera vez notebook\n",
    "!pip install --upgrade pip\n",
    "!pip install jupyter\n",
    "!pip install tensorflow==1.14\n",
    "#!conda install pandas\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación \n",
    "### 1.1 Importación de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "print(tf.__version__)\n",
    "from tensorflow.python.framework import ops\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Importación de datasets\n",
    "Se importan los datasets previamente rescatados, estos deben ser guardados en la carpeta raiz de notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La dimension de set de datos de entrada es:  (53, 9904)\n",
      "La dimension de set de datos de salida es:  (1, 9904)\n"
     ]
    }
   ],
   "source": [
    "#Cargar datasets para entrenamiento\n",
    "#print(os.getcwd())\n",
    "#print(os.listdir())\n",
    "X_mean=[]\n",
    "X_Orig = pd.read_csv('/home/jovyan/Datos/X_Train_Jul2019.csv',delimiter=';',engine='python')\n",
    "Y_Orig = pd.read_csv('/home/jovyan/Datos/Y_Train_Jul2019.csv',delimiter=';',engine='python')\n",
    "#print(np.sum(X_Train.isnull()))\n",
    "X_Orig=X_Orig.values\n",
    "Y_Orig=Y_Orig.values\n",
    "X_Orig=X_Orig.astype(np.float).T\n",
    "Y_Orig=Y_Orig.astype(np.float).T\n",
    "X_Orig=np.delete(X_Orig,53,0)\n",
    "\n",
    "print(\"La dimension de set de datos de entrada es: \",X_Orig.shape)\n",
    "print(\"La dimension de set de datos de salida es: \",Y_Orig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Normalización set de datos\n",
    "La normalización de datos tiene por objetivo normalizar a por unidad todos los datos de entrada, es decir en rango de $[0,1]$ para extraer efectivamente la información de cada una de las variables alimentada al modelo.\n",
    "\n",
    "Para la normalización de las variables se utilizaron dos formas:\n",
    "1. Normalización de datos:\n",
    "$$X_{norm}=\\frac{(X-X_{Min})}{(X_{Max}-X_{Min})}\\tag{1}$$\n",
    "2. Estandarización de datos\n",
    "$$X_{std}=\\frac{(X-\\bar{X})}{\\sigma}\\tag{2}$$\n",
    " (Desviación estandar): $ \\sigma = \\sqrt[]{\\frac{1}{N-1}\\sum_{i=1}^{N}X_i-\\bar{X}}\\tag{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La dimension de los datos originales es (53, 9904)\n",
      "La dimension de los datos normalizados es (53, 9904)\n",
      "La dimension de los datos estandarizados es (53, 9904)\n",
      "La dimension de los datos entrenamiento es (53, 6932)\n",
      "La dimension de los datos test es (53, 1485)\n",
      "La dimension de los datos dev es (53, 1485)\n"
     ]
    }
   ],
   "source": [
    "X_max=np.zeros((X_Orig.shape[0],1))\n",
    "X_min=np.zeros((X_Orig.shape[0],1))\n",
    "\n",
    "\n",
    "i=0\n",
    "\n",
    "mean=np.reshape(np.mean(X_Orig,axis=1),(53,1),order='F')\n",
    "var=np.reshape(np.var(X_Orig,axis=1),(53,1),order='F')\n",
    "devstd=np.reshape(np.std(X_Orig,axis=1),(53,1),order='F')\n",
    "\n",
    "cont=X_Orig.shape[0]\n",
    "sample_cont=X_Orig.shape[1]\n",
    "\n",
    "train_cont=int(np.round(sample_cont*0.7))\n",
    "test_cont=int(np.round((sample_cont-1)*0.15))\n",
    "dev_cont=int(np.round((sample_cont-1)*0.15))\n",
    "\n",
    "for i in range(cont):\n",
    "        X_max[i]=np.max(X_Orig[[i]])\n",
    "        X_min[i]=np.min(X_Orig[[i]])\n",
    "\n",
    "X_gain=X_max-X_min\n",
    "X_norm=np.divide(X_Orig-X_min,X_gain)\n",
    "X_std=np.divide((X_Orig-mean),devstd)\n",
    "#X_train=[[53,int(train_cont)]]\n",
    "#Y_train=[[53,int(train_cont)]]\n",
    "\n",
    "X_train=X_std[:,1:train_cont]\n",
    "Y_train=Y_Orig[:,1:train_cont]\n",
    "X_test=X_std[:,train_cont+1:test_cont+train_cont+1]\n",
    "Y_test=Y_Orig[:,train_cont+1:test_cont+train_cont+1]\n",
    "X_dev=X_std[:,test_cont+train_cont+1:dev_cont+test_cont+train_cont+1]\n",
    "Y_dev=Y_Orig[:,test_cont+train_cont+1:dev_cont+test_cont+train_cont+1]\n",
    "\n",
    "\n",
    "#print(np.max(X_Train[[52]]))\n",
    "#print(np.min(X_Train[[52]]))\n",
    "#print(\"X_max: \",X_max[[52]])\n",
    "#print(\"X_min: \",X_min[[52]])\n",
    "\n",
    "print(\"La dimension de los datos originales es\",X_Orig.shape)\n",
    "print(\"La dimension de los datos normalizados es\",X_norm.shape)\n",
    "print(\"La dimension de los datos estandarizados es\",X_std.shape)\n",
    "print(\"La dimension de los datos entrenamiento es\",X_train.shape)\n",
    "print(\"La dimension de los datos test es\",X_test.shape)\n",
    "print(\"La dimension de los datos dev es\",X_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creación de Minibatch\n",
    "Para acelerar los entrenamientos se crean sets de datos mas pequeños de forma que no es necesario explorar el sets de datos completos para obtener una aproximacion de las matrices de pesos y bias, y a cada exploracion de minibatch se obtiene una optimización de los parametros de la red neuronal.\n",
    "El ideal es que los tamaños de los minibatches sea de numero par consistentes con cuentas binarias (32,64,128,512...) de manera de optimizar el tamaño de uso de memoria RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobación de correcto funcionamiento de función\n",
    "\n",
    "```Random_mini_batches(X_train,Y_Train,32,1)```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the 1st mini_batch_X: (53, 32)\n",
      "shape of the 2nd mini_batch_X: (53, 32)\n",
      "shape of the 3rd mini_batch_X: (53, 32)\n",
      "shape of the 1st mini_batch_Y: (1, 32)\n",
      "shape of the 2nd mini_batch_Y: (1, 32)\n",
      "shape of the 3rd mini_batch_Y: (1, 32)\n",
      "mini batch check: [ 0.08687692 -7.92478671  0.11012979]\n"
     ]
    }
   ],
   "source": [
    "mini_batches = random_mini_batches(X_test, Y_test,32,1)\n",
    "print (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
    "print (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
    "print (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
    "print (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\n",
    "print (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \n",
    "print (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\n",
    "print (\"mini batch check: \" + str(mini_batches[0][0][0][0:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creación Red Neuronal\n",
    "\n",
    "### 2.1 Función para creación de vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Crea los placeholders para la sesion de tensorflow.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- escalar, tamaño de variables de entradas\n",
    "    n_y -- escalar, numero de calisificadores (caso particular de detección corte de hoja 1)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder para los datos de entrada, de dimension [n_x, None] y dtype \"float\"\n",
    "    Y -- placeholder para las etiquetas de salida, de dimensión [n_y, None] y dtype \"float\"\n",
    "    \"\"\"\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(n_x,None))\n",
    "    Y = tf.placeholder(tf.float32,  shape=(n_y,None))\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Función de inicialización de parametros\n",
    "Se inicializan las matrices de peso y bias de la red neuronal.\n",
    "En este bloque es donde se indica la cantidad de neuronas que tendra cada capa.\n",
    "\n",
    "Inicialmente la cantidad de neuronas en las capas ocultas es la siguiente:\n",
    "\n",
    "* $Capa_1: 15$\n",
    "* $Capa_2: 15$\n",
    "* $Capa_3: 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Inicializa los parametros para crear la red neuronal en tensorflow. Las dimensiones son:\n",
    "                        W1 : [15, 53]\n",
    "                        b1 : [15, 1]\n",
    "                        W2 : [15, 15]\n",
    "                        b2 : [15, 1]\n",
    "                        W3 : [1, 15]\n",
    "                        b3 : [1, 1]\n",
    "    \n",
    "    Salida:\n",
    "    parameters -- dictionario que contiene W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)\n",
    "    W1 =tf.get_variable(\"W1\", [15,53], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [15,1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [6,15], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [6,1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [1,6], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable(\"b3\", [1,1], initializer = tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Función Feedforward\n",
    "\n",
    "Se construye red neuronal con funciones de activación sigmoidales de tres capas:\n",
    "<img src=\"Imagenes/Arquitectura Red.png\">\n",
    "\n",
    "---\n",
    "\n",
    "Matematicamente es:\n",
    "\n",
    "$$Z^{[1]}=W^{[1]}*X_{norm}+B_{[1]}\\small\\tag{1}$$\n",
    "$$Z^{[i]}=W^{[i]}*Z^{[i]}+B_{[i]}\\small\\tag{2}$$\n",
    "$$A^{[i]}=sigmoid(Z^{[i]})=\\sigma(Z^{[i]})\\small\\tag{3}$$\n",
    "$$\\hat{y}=\\sigma(Z^{[3]})\\small\\tag{4}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implementación de forward propagation: LINEAR -> SIGMOID -> LINEAR -> SIGMOID -> LINEAR -> SIGMOID\n",
    "    \n",
    "    Arguments:\n",
    "    X -- entrada dataset placeholder, de dimension (tamaño entrada, numero de muestras)\n",
    "    parameters -- contiene los parametros \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  las dimensiones son dadas por la función de inicialización\n",
    "\n",
    "    Salida:\n",
    "    Z3 -- salida de la ultima función LINEAR\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rescata los parametros desde el diccionario \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    Z1 = tf.add(tf.matmul(W1,X),b1)\n",
    "    A1 = tf.nn.sigmoid(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2,A1),b2)\n",
    "    A2 = tf.nn.sigmoid(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3,A2),b3)\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Función Costo\n",
    "\n",
    "La función de costo utilizada es:\n",
    "$$ J = - \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log a^{ [2] (i)} + (1-y^{(i)})\\log (1-a^{ [2] (i)} )\\large )\\small\\tag{5}$$\n",
    "La función de costo (5) es conocida como \"*sigmoid cross entropy*\" y en tensorflow puede ser llamada como:\n",
    "\n",
    "```tf.nn.sigmoid_cross_entropy_with_logits()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Calculo costo\n",
    "    \n",
    "    Argumentos:\n",
    "    Z3 -- salida de forward propagation (salida de la ultima función LINEAR), de dimension (1, numero de muestras)\n",
    "    Y -- \"true\" vector de etiquetas placeholder, misma dimension que Z3\n",
    "    \n",
    "    Salida:\n",
    "    cost - Tensor de la función de costo\n",
    "    \"\"\"\n",
    "    \n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Construcción del Modelo\n",
    "Se utilizan las funciones generadas antes para construir el modelo de la red neuronal y generar el entrenamiento utilizando los minibatch ya generados. Los parametros que pueden ser modificados para la busqueda de mejor comportamiento del modelo son los siguientes:\n",
    "* learning rate ```learning_rate```: tamaño de la corrección por cada iteración obtenida desde el algoritmo Adam.\n",
    "* numero de epocas ```num_epochs```: numero de veces que explorará todos los minibatches.\n",
    "* tamaño de minibatch ```minibatch_size```: tamaño de los minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.001,\n",
    "          num_epochs = 100, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implementa tres-capas tensorflow red neuronal: LINEAR->SIGMOID->LINEAR->SIGMOID->LINEAR->SIGMOID.\n",
    "    \n",
    "    Argumentos:\n",
    "    X_train -- training set, de dimension (tamaño entradas = 53, numero de muestras = 1080)\n",
    "    Y_train -- test set, de dimension (tamaño salida = 1, numero de muestras = 1080)\n",
    "    X_test -- training set, de dimension (tamaño entradas = 53, numero de muestras = 1080)\n",
    "    Y_test -- test set, de dimension (tamaño salida = 1, numero de muestras = 1080)\n",
    "    learning_rate -- learning rate de la optimización\n",
    "    num_epochs -- number de epocas de ciclo de optimización\n",
    "    minibatch_size -- tamaño minibatch\n",
    "    print_cost -- True para imprimir el costo cada 100 epocas\n",
    "    \n",
    "    Salida:\n",
    "    parameters -- parametros aprendidos por el modelos.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # para correr modelo sin sobreescribir tf variables\n",
    "    tf.set_random_seed(1)                             # para mantener resultados consistentes\n",
    "    seed = 3                                          # para mantener resultados consistentes\n",
    "    (n_x, m) = X_train.shape                          # (n_x: tamaño set entradas, m : numeros de muestras en el dataset)\n",
    "    n_y = Y_train.shape[0]                            # n_y : tamaño de salida\n",
    "    costs = []                                        # para mantener seguimiento de función de costo\n",
    "    print(\"n_x: \",n_x)\n",
    "    print(\"n_y: \",n_y)\n",
    "    # Crea variables de tf de orden (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x,n_y)\n",
    "\n",
    "    # Inicializa parametros\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    # Forward propagation: construye forward propagation en el grafico de tensorflow\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    \n",
    "    # Función de costo: Incluye la función de costo en el grafico de tensorflow\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    \n",
    "    # Backpropagation: Define el optimizador de tensorflow. Usa AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # Inicializa todas las variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Inicia la sesión para computar el grafico de tensorflow\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Corre initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Realiza training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "        \n",
    "                # Selecciona minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                #print(minibatch_X[1][0])\n",
    "                \n",
    "                #IMPORTANTE: Función que ejecuta optimización en tensorflow\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Muestra el costo en cada epoca\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # grafica el costo\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('costo')\n",
    "        plt.xlabel('iteraciones (x10)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # Guarda los parametros en una variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Los parametros han sido entrenados!\")\n",
    "\n",
    "        # Calcula las predicciones correctas\n",
    "        correct_prediction = tf.equal(tf.cast(tf.greater_equal(tf.sigmoid(Z3),0.5), \"bool\"), tf.cast(tf.argmax(Y), \"bool\"))\n",
    "\n",
    "        # Calcula el accuracy en el test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        W1 = parameters['W1']\n",
    "        b1 = parameters['b1']\n",
    "        W2 = parameters['W2']\n",
    "        b2 = parameters['b2']\n",
    "        W3 = parameters['W3']\n",
    "        b3 = parameters['b3']\n",
    "        print(\"W1= \", W1)\n",
    "        print(\"b1= \", b1)\n",
    "        print(\"W2= \", W2)\n",
    "        print(\"b2= \", b2)\n",
    "        print(\"W3= \", W3)\n",
    "        print(\"b3= \", b3)\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Entrenamiento Red Neuronal\n",
    "Se entrena la red neuronal utilizando el set de datos normalizado, como resultado se obtendra:\n",
    "1. Cada 100 epocas imprimira el valor de la función de costo que se esta optimizando\n",
    "2. Al finalizar la iteración por la cantidad de epocas imprimira el grafico de evolución de la función de costo\n",
    "3. Imprimira el valor de cada una de las matrices de pesos y bias de cada capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_x:  53\n",
      "n_y:  1\n",
      "Cost after epoch 0: 0.714516\n"
     ]
    }
   ],
   "source": [
    "#parameters = model(X_train_norm, Y_Train, X_train_norm, Y_Train)\n",
    "parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Validación Modelo\n",
    "Se genera funcion para validar modelo entrenado con distintos sets de datos para verificar la correcta predicción, para lo anterior se genera la función ```predict()``` que utiliza la función de modelo de red neuronal ya creado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters, hys):\n",
    "    \n",
    "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
    "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
    "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
    "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
    "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
    "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
    "    \n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "    (n_x, m) = X.shape \n",
    "    x = tf.placeholder(\"float\", [n_x, m])\n",
    "    \n",
    "    z3 = forward_propagation(x, params)\n",
    "    #p = tf.argmax(tf.sigmoid(z3))\n",
    "    p = tf.greater_equal(tf.sigmoid(z3),hys)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        prediction = sess.run(p, feed_dict = {x: X})\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ejecuta la función predict con los sets de datos X_train, X_test y X_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "y_hat=predict(X_train, parameters,0.5)\n",
    "plt.plot(np.squeeze(y_hat), label='Modelo')\n",
    "plt.plot(np.squeeze(Y_train), label='Y_train')\n",
    "plt.ylabel('costo')\n",
    "plt.xlabel('muestras')\n",
    "plt.title(\"Predicciones vs Real\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(tf.argmax(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
